# Sign2Text - Vision Transformer Powered Video Processing

**Sign Language requires precise video modality data processing for better accuracy.**  
We propose a **Vision Transformer-based Video Processing Method** where **each frame of a video is treated as a patch** of a larger image, akin to the idea:

### "A frame of a Video is worth 128\*128 words" üòÅüòÇ

This approach allows parallel processing of frames, enabling more efficient and accurate sign language recognition. üöÄ

## üì∫ Watch the Demo Video
[![Watch the video](https://img.youtube.com/vi/2n_BUWRjtJI/0.jpg)](https://youtu.be/2n_BUWRjtJI)

Click the image above or [this link](https://youtu.be/2n_BUWRjtJI) to see the video in action!

## üî¢ Dataset
### Sign Language ViT Dataset
Our custom dataset includes 128-frame video clips (4-5 seconds each) from YouTube, the INCLUDE (Indian Lexicon Sign Language Dataset), and other sources. The dataset covers 351 sign language classes, providing a comprehensive foundation for training.
 Link for dataset: [Sign Language ViT Dataset](https://www.kaggle.com/datasets/tonystark213123/sign-language-vit-dataset) 

 ### **Data Preprocessing**  
- Each video is processed **frame by frame** using **OpenCV** and combined into a single image, with each frame represented as a patch.
- **MediaPipe** is employed for advanced pose estimation to capture detailed hand movements, facial expressions, and spatial dynamics.
- After extracting landmarks using MediaPipe, background masking techniques are applied to isolate hand movements, reducing noise and ensuring accurate classification.

### **Fine-Tuning Phi-1.5B**  
The **PHI-1.5B** language model was fine-tuned using **900 sentence pairs** generated by **Gemini API**, representing a wide range of linguistic errors such as typos, grammar errors, and incomplete sentences. This ensures **PHI-1.5B** produces accurate and contextually relevant translations from sign language gestures classified by ViT.

---

## How it Works:
- **Vision Transformer (ViT)**: Treats each video frame as a patch for parallel processing.
- **Patch-based Approach**: Each frame becomes a critical component, helping the model understand sign language gestures in depth.
- **Why ViT?**: ViTs excel at learning spatial and temporal patterns, making them ideal for sign language processing.


---
## üèÜ Conference Submission
This work has been submitted to the **2024 IEEE International Conference on Artificial Intelligence in Engineering and Technology (IICAIET 2024)**. It showcases the potential of Vision Transformers combined with advanced language models in sign language translation.

---

## üìä Results and Graphs

- **Training Accuracy**: The model achieved **80% accuracy** after 100 epochs on 351 word classes.
- **Training Loss**: The training loss decreased consistently, showcasing progressive learning and refinement over time.

---
