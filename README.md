---
# Sign2Text - Vision Transformer Powered Video Processing

**Sign Language requires precise video modality data processing for better accuracy.**  
We propose a **Vision Transformer-based Video Processing Method** where **each frame of a video is treated as a patch** of a larger image, akin to the idea:

### "A frame of a Video is worth 128\*128 words" ğŸ˜ğŸ˜‚

This approach allows parallel processing of frames, enabling more efficient and accurate sign language recognition. ğŸš€

## ğŸ“º Watch the Demo Video
[![Watch the video](https://img.youtube.com/vi/2n_BUWRjtJI/0.jpg)](https://youtu.be/2n_BUWRjtJI)

Click the image above or [this link](https://youtu.be/2n_BUWRjtJI) to see the video in action!

## How it Works:
- **Vision Transformer (ViT)**: Treats each video frame as a patch for parallel processing.
- **Patch-based Approach**: Each frame becomes a critical component, helping the model understand sign language gestures in depth.
- **Why ViT?**: ViTs excel at learning spatial and temporal patterns, making them ideal for sign language processing.

---
